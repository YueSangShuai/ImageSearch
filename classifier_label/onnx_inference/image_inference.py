from transformers import AutoImageProcessor
from PIL import Image
import numpy as np
import onnxruntime as ort
import time

# === é…ç½® ===
onnx_path = "/data/yuesang/LLM/VectorIE/models/nomic/nomic-embed-vision-v1.5/onnx/model_uint8.onnx"  # æ›¿æ¢ä¸ºä½ çš„ ONNX æ¨¡å‹è·¯å¾„
model_path = "/data/yuesang/LLM/VectorIE/models/nomic/nomic-embed-vision-v1.5"       # ç”¨äºåŠ è½½é¢„å¤„ç†å™¨
device = "cpu"  # or "cpu"
image_path = "/data/yuesang/LLM/VectorIE/classifier_label/image-3.png"  # æ›¿æ¢ä¸ºä½ çš„å›¾åƒè·¯å¾„

# === å›¾åƒé¢„å¤„ç† ===
image = Image.open(image_path).convert("RGB")
processor = AutoImageProcessor.from_pretrained(model_path)
inputs = processor(images=image, return_tensors="np")  # -> {"pixel_values": np.array of shape (1, 3, 224, 224)}

# === åˆ›å»º ONNX session ===
providers = ["CUDAExecutionProvider"] if device == "cuda" else ["CPUExecutionProvider"]
session = ort.InferenceSession(onnx_path, providers=providers)

input_name = session.get_inputs()[0].name   # é€šå¸¸æ˜¯ 'pixel_values'
output_name = session.get_outputs()[0].name # é€šå¸¸æ˜¯ 'embeddings' or 'last_hidden_state'

onnx_inputs = {input_name: inputs["pixel_values"]}


warn_up_range=50
test_range=500
from tqdm import tqdm


ouput = session.run([output_name], onnx_inputs)
print(len(ouput))
print(ouput[0].shape)

# === é¢„çƒ­ ===
for _ in tqdm(range(warn_up_range)):
    _ = session.run([output_name], onnx_inputs)

# === æ­£å¼æ¨ç† + è®¡æ—¶ ===
start = time.time()
for _ in tqdm(range(test_range)):
    _ = session.run([output_name], onnx_inputs)
end = time.time()

avg_time = (end - start) / test_range
print(f"ğŸŒ¡ï¸ å¹³å‡æ¨ç†è€—æ—¶: {avg_time * 1000:.2f} ms")

# === è·å–æœ€ç»ˆ embedding ===
output = session.run([output_name], onnx_inputs)
embedding = output[0]  # (1, 768)

